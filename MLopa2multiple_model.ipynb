{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08069b31",
   "metadata": {},
   "source": [
    "# working in MLOps with multiple classification models and tracking results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57c30911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bcc7dd",
   "metadata": {},
   "source": [
    "# The code below  generates a binary classification dataset with a 90:10 class imbalance using make_classification. The output np.unique(y, return_counts=True) confirms the distribution of samples across the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b202409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([900, 100]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = make_classification(n_samples=1000, n_features=10, n_informative=2, n_redundant=8, \n",
    "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
    "\n",
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff033a",
   "metadata": {},
   "source": [
    "Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70e1aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test  =train_test_split(x,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb0fae",
   "metadata": {},
   "source": [
    "# Training with Logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f15019f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       270\n",
      "           1       0.78      0.60      0.68        30\n",
      "\n",
      "    accuracy                           0.94       300\n",
      "   macro avg       0.87      0.79      0.82       300\n",
      "weighted avg       0.94      0.94      0.94       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(C=1,solver='liblinear')\n",
    "log_reg.fit(x_train,y_train)\n",
    "y_pred_log_reg  =log_reg.predict(x_test)\n",
    "print(classification_report(y_test,y_pred_log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d60b1",
   "metadata": {},
   "source": [
    "# Training with Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3607ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       270\n",
      "           1       1.00      0.63      0.78        30\n",
      "\n",
      "    accuracy                           0.96       300\n",
      "   macro avg       0.98      0.82      0.88       300\n",
      "weighted avg       0.96      0.96      0.96       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=30,max_depth=3)\n",
    "rf_clf.fit(x_train,y_train)\n",
    "y_pred_rf = rf_clf.predict(x_test)\n",
    "print(classification_report(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc0c14",
   "metadata": {},
   "source": [
    "# Training with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fb68627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       270\n",
      "           1       0.88      0.77      0.82        30\n",
      "\n",
      "    accuracy                           0.97       300\n",
      "   macro avg       0.93      0.88      0.90       300\n",
      "weighted avg       0.97      0.97      0.97       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = XGBClassifier(use_label_encoder=False,eval_metric= 'logloss')\n",
    "xgb_clf.fit(x_train,y_train)\n",
    "y_pred_xgb = xgb_clf.predict(x_test)\n",
    "print(classification_report(y_test,y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90088d6",
   "metadata": {},
   "source": [
    "# USING SMOTE\n",
    "# below code applies SMOTETomek, which combines over-sampling of the minority class using SMOTE and under-sampling of the majority class using Tomek Links. It helps to balance the class distribution in the training data. The np.unique function shows the updated sample counts for each class after resampling.\n",
    "# Handel class imbalance using SMOTETomek then again train dataset with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bc062bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([614, 614]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smt = SMOTETomek(random_state=0)\n",
    "x_train_res,y_train_res = smt.fit_resample(x_train,y_train)\n",
    "\n",
    "np.unique(y_train_res,return_counts=True)\n",
    "\n",
    "#Import SMOTETomek: Use from imblearn.combine import SMOTETomek to bring in the technique that combines SMOTE and Tomek Links for balancing class distribution.\n",
    "#Initialize SMOTETomek: Create an instance with a fixed random state for reproducibility, e.g., smote_tomek = SMOTETomek(random_state=42).\n",
    "#Apply fit_resample: Use X_train_res, y_train_res = smote_tomek.fit_resample(X_train, y_train) to generate a new balanced dataset.\n",
    "#Check Class Distribution: Use np.unique(y_train_res, return_counts=True) to verify the new class distribution after resampling.\n",
    "#Confirm Balance: The output should show equal counts for both classes, indicating successful balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f044e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       270\n",
      "           1       0.83      0.83      0.83        30\n",
      "\n",
      "    accuracy                           0.97       300\n",
      "   macro avg       0.91      0.91      0.91       300\n",
      "weighted avg       0.97      0.97      0.97       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = XGBClassifier(use_label_encoder=False,eval_metric='logloss')\n",
    "xgb_clf.fit(x_train_res,y_train_res)\n",
    "y_pred_xgb = xgb_clf.predict(x_test)\n",
    "print(classification_report(y_test,y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9493ac5",
   "metadata": {},
   "source": [
    "#  Tracking  all 4 above Experiments using MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c3ca1",
   "metadata": {},
   "source": [
    "# Models that are used below  Explained:\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "A linear model for binary classification.\n",
    "\n",
    "C=1: Regularization parameter (higher = less regularization).\n",
    "\n",
    "solver='liblinear': Good for small datasets and binary classification.\n",
    "\n",
    "Random Forest\n",
    "\n",
    "An ensemble of decision trees using bagging.\n",
    "\n",
    "n_estimators=30: Builds 30 trees.\n",
    "\n",
    "max_depth=3: Limits tree depth to reduce overfitting and improve generalization.\n",
    "\n",
    "XGBoost Classifier\n",
    "\n",
    "A powerful gradient boosting algorithm.\n",
    "\n",
    "use_label_encoder=False: Disables deprecated label encoder (avoids warnings).\n",
    "\n",
    "eval_metric='logloss': Uses log loss to evaluate performance during training.\n",
    "\n",
    "XGBoost Classifier With SMOTE\n",
    "\n",
    "Same model as above but trained on resampled (balanced) data using SMOTETomek.\n",
    "\n",
    "(x_train_res, y_train_res): Balanced training data after applying SMOTE + Tomek Links.\n",
    "\n",
    "Helps improve prediction performance on imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29b41619",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\n",
    "        \"Logistic Regression\", \n",
    "        LogisticRegression(C=1, solver='liblinear'), \n",
    "        (x_train, y_train),\n",
    "        (x_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"Random Forest\", \n",
    "        RandomForestClassifier(n_estimators=30, max_depth=3), \n",
    "        (x_train, y_train),\n",
    "        (x_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"XGBClassifier\",\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='logloss'), \n",
    "        (x_train, y_train),\n",
    "        (x_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"XGBClassifier With SMOTE\",\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='logloss'), \n",
    "        (x_train_res, y_train_res),\n",
    "        (x_test, y_test)\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1605c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = []\n",
    "\n",
    "for model_name, model, train_set, test_set in models:\n",
    "    x_train = train_set[0]\n",
    "    y_train = train_set[1]\n",
    "    x_test = test_set[0]\n",
    "    y_test = test_set[1]\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    reports.append(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fff4e3",
   "metadata": {},
   "source": [
    "#  Above Loop Breakdown (Line-by-Line):\n",
    "\n",
    "for model_name, model, train_set, test_set in models:\n",
    "\n",
    "Iterates through each model tuple defined earlier.\n",
    "\n",
    "Unpacks the model name, the model object, and train/test datasets.\n",
    "\n",
    "X_train = train_set[0] and y_train = train_set[1]\n",
    "\n",
    "Extracts the training features and labels.\n",
    "\n",
    "X_test = test_set[0] and y_test = test_set[1]\n",
    "\n",
    "Extracts the test features and labels.\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "Trains the model using the training data.\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "Uses the trained model to predict labels on the test set.\n",
    "\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "Generates a detailed performance report as a dictionary.\n",
    "\n",
    "Includes metrics like precision, recall, F1-score, and support.\n",
    "\n",
    "reports.append(report)\n",
    "\n",
    "Appends the classification report to the reports list for later use or comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fa9cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow #Brings in MLflow, a tool to track and manage machine learning projects.\n",
    "import mlflow.sklearn #Adds support to save and track scikit-learn models with MLflow.\n",
    "import mlflow.xgboost #Adds support to save and track XGBoost models with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e3ac9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/22 12:56:13 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/22 12:56:17 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Logistic Regression at: http://localhost:5000/#/experiments/816397754637035625/runs/1e0e892a9eff410e89a3fca6dab2bfde\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/816397754637035625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/22 12:56:18 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/22 12:56:22 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Random Forest at: http://localhost:5000/#/experiments/816397754637035625/runs/3d5a9685979a4ae1bd73f7e6d6a0cf5e\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/816397754637035625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/22 12:56:22 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/22 12:56:28 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run XGBClassifier at: http://localhost:5000/#/experiments/816397754637035625/runs/2efb1466c0904ba28d39322d85c9ba72\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/816397754637035625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/22 12:56:28 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/22 12:56:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run XGBClassifier With SMOTE at: http://localhost:5000/#/experiments/816397754637035625/runs/3a6497351abe41c8b4e9c88023cd2ac3\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/816397754637035625\n"
     ]
    }
   ],
   "source": [
    "# Initializing MLFLOW\n",
    "mlflow.set_experiment(\"imbalanced classification\")\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "for i, element in enumerate(models):\n",
    "    model_name = element[0]\n",
    "    model = element[1]\n",
    "    report = reports[i]\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name):        \n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        mlflow.log_metric('accuracy', report['accuracy'])\n",
    "        mlflow.log_metric('recall_class_1', report['1']['recall'])\n",
    "        mlflow.log_metric('recall_class_0', report['0']['recall'])\n",
    "        mlflow.log_metric('f1_score_macro', report['macro avg']['f1-score'])        \n",
    "        \n",
    "        if \"XGB\" in model_name:\n",
    "            mlflow.xgboost.log_model(model, \"model\")\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, \"model\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
